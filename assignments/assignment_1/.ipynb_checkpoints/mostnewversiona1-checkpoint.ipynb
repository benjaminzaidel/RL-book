{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stanford CME 241 (Winter 2025) - Assignment 1\n",
    "\n",
    "**Due: Sunday, January 19 @ 11:59 PM PST on Gradescope.**\n",
    "\n",
    "Assignment instructions:\n",
    "- Make sure each of the subquestions have answers\n",
    "- Ensure that group members indicate which problems they're in charge of\n",
    "- Show work and walk through your thought process where applicable\n",
    "- Empty code blocks are for your use, so feel free to create more under each section as needed\n",
    "- Document code with light comments (i.e. 'this function handles visualization')\n",
    "\n",
    "Submission instructions:\n",
    "- When complete, fill out your publicly available GitHub repo file URL and group members below, then export or print this .ipynb file to PDF and upload the PDF to Gradescope.\n",
    "\n",
    "*Link to this ipynb file in your public GitHub repo (replace below URL with yours):* \n",
    "\n",
    "https://github.com/benjaminzaidel/RL-book/blob/master/assignments/assignment_1/mostnewversiona1.ipynb\n",
    "\n",
    "*Group members (replace below names with people in your group):* \n",
    "- Benjamin Zaidel\n",
    "- Ohm Patel\n",
    "- Stephen Li"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1: Snakes and Ladders (Led by Ohm)\n",
    "\n",
    "In the classic childhood game of Snakes and Ladders, all players start to the left of square 1 (call this position 0) and roll a 6-sided die to represent the number of squares they can move forward. The goal is to reach square 100 as quickly as possible. Landing on the bottom rung of a ladder allows for an automatic free-pass to climb, e.g. square 4 sends you directly to 14; whereas landing on a snake's head forces one to slide all the way to the tail, e.g. square 34 sends you to 6. Note, this game can be viewed as a Markov Process, where the outcome is only depedent on the current state and not the prior trajectory. In this question, we will ask you to both formally describe the Markov Process that describes this game, followed by coding up a version of the game to get familiar with the RL-book libraries.\n",
    "\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we model this problem with a Markov Process?\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Formalize the state space of the Snakes and Ladders game. Don't forget to specify the terminal state!\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Transition Probabilities\n",
    "\n",
    "Write out the structure of the transition probabilities. Feel free to abbreviate all squares that do not have a snake or ladder.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Modeling the Game\n",
    "\n",
    "Code up a `transition_map: Transition[S]` data structure to represent the transition probabilities of the Snakes and Ladders Markov Process so you can model the game as an instance of `FiniteMarkovProcess`. Use the `traces` method to create sampling traces, and plot the graph of the distribution of time steps to finish the game. Use the image below for the locations of the snakes and ladders.\n",
    "\n",
    "![Snakes and Laddders](./Figures/snakesAndLadders.png)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">State space S = {0, 1, 2, ..., 100}. Terminal set T = {100}, as the game ends once you reach square 100. Initial state is 0.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For any state \\(s \\in S\\), the transition probabilities are given as follows:\n",
    "\n",
    "\\[\n",
    "P(S_{t+1} = s') =\n",
    "\\begin{cases} \n",
    "\\frac{1}{6}, & \\text{if } s < 100 \\text{ and } s' = \\text{snakeLadderUpdate}(s + k), \\text{ where } k \\in \\{1, 2, \\dots, 6\\}, \\\\\n",
    "1.0, & \\text{if } s = 100 \\text{ and } s' = 100, \\\\\n",
    "0, & \\text{otherwise}.\n",
    "\\end{cases}\n",
    "\\]\n",
    "\n",
    "Here, the function \\(\\text{snakeLadderUpdate}(x)\\) is defined as:\n",
    "\n",
    "\\[\n",
    "\\text{snakeLadderUpdate}(x) =\n",
    "\\begin{cases} \n",
    "x, & \\text{if } x \\text{ is not affected by a snake or ladder}, \\\\\n",
    "x', & \\text{if } x \\text{ is affected by a snake or ladder and transitions to } x'.\n",
    "\\end{cases}\n",
    "\\]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append(\"/Users/ohmpatel/Desktop//CME241/RL-book/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Mapping\n",
    "from rl.markov_process import FiniteMarkovProcess, Transition, NonTerminal, Terminal\n",
    "from rl.distribution import Categorical\n",
    "from typing import Dict\n",
    "\n",
    "\n",
    "\n",
    "class SnakesAndLadders(FiniteMarkovProcess[int]):\n",
    "    def __init__(self, snakes_and_ladders: Dict[int, int]) -> None:\n",
    "        self.snakes_and_ladders = snakes_and_ladders\n",
    "        super().__init__(transition_map=self.transition_map())\n",
    "\n",
    "\n",
    "    def transition_map(self) -> Transition[int]:\n",
    "        \"\"\"\n",
    "        Returns a dictionary: state -> Categorical(next_state),\n",
    "        specifying transition probabilities from every state (0..100).\n",
    "        \"\"\"\n",
    "        d: Dict[int, Categorical[int]] = {}\n",
    "\n",
    "        # For states 0 through 99:\n",
    "        for s in range(100):\n",
    "            next_states: Dict[int, float] = {}\n",
    "            for roll in range(1, 7):\n",
    "                raw_next = s + roll\n",
    "                if raw_next >= 100:\n",
    "                    # Over 100, sit at 100\n",
    "                    final_pos = 100\n",
    "                else:\n",
    "                    # Apply any snake or ladder\n",
    "                    final_pos = self.snakes_and_ladders.get(raw_next, raw_next)\n",
    "\n",
    "                next_states[final_pos] = next_states.get(final_pos, 0.0) + 1/6\n",
    "\n",
    "            d[s] = Categorical(next_states)\n",
    "\n",
    "        # State 100 is terminal: it transitions to itself with probability 1.0\n",
    "        d[100] = Categorical({Terminal(100): 1.0})\n",
    "\n",
    "        return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakesAndLaddersMap = {1: 38, 4: 14, 8: 30,\n",
    "                       21: 42, 28: 76, 32: 10, \n",
    "                       36: 6, 48: 26, 50: 67,\n",
    "                       62: 18, 71: 92, 80: 99,\n",
    "                       88: 24, 95: 56, 97: 78\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "snakes_and_ladders_process = SnakesAndLadders(snakesAndLaddersMap)\n",
    "start_state = Categorical({NonTerminal(0): 1.0})\n",
    "traces = snakes_and_ladders_process.traces(start_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "trace_lens = [len([x.state for x in next(traces)]) for _ in range(50000)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "results = dict(Counter(trace_lens))\n",
    "keys = list(results.keys())\n",
    "frequencies = list(results.values())\n",
    "\n",
    "# Plot the histogram\n",
    "plt.bar(keys, frequencies, color='blue', edgecolor='black', alpha=0.7)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Step Count')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Histogram of Step Frequencies')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2: Markov Decision Processes (Led by Benjamin)\n",
    "\n",
    "Consider an MDP with an infinite set of states $\\mathcal{S} = \\{1,2,3,\\ldots \\}$. The start state is $s=1$. Each state $s$ allows a continuous set of actions $a \\in [0,1]$. The transition probabilities are given by: \n",
    "$$\\mathbb{P}[s+1 \\mid s, a] = a, \\mathbb{P}[s \\mid s, a] = 1 - a \\text{ for all } s \\in \\mathcal{S} \\text{ for all } a \\in [0,1]$$\n",
    "For all states $s \\in \\mathcal{S}$ and actions $a \\in [0,1]$, transitioning from $s$ to $s+1$ results in a reward of $1-a$ and transitioning from $s$ to $s$ results in a reward of $1+a$. The discount factor $\\gamma=0.5$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "How can we derive a mathematical formulation for the value function and the optimal policy? And how do those functions change when we modify the action space?\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): Optimal Value Function  \n",
    "\n",
    "Using the MDP Bellman Optimality Equation, calculate the Optimal Value Function $V^*(s)$ for all $s \\in \\mathcal{S}$. Given $V^*(s)$, what is the optimal action, $a^*$, that maximizes the optimal value function?\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Optimal Policy  \n",
    "\n",
    "Calculate an Optimal Deterministic Policy $\\pi^*(s)$ for all $s \\in \\mathcal{S}$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Changing the Action Space  \n",
    "\n",
    "Let's assume that we modify the action space such that instead of $a \\in [0,1]$ for all states, we restrict the action space to $a \\in \\left[0,\\frac{1}{s}\\right]$ for state $s$. This means that higher states have more restricted action spaces. How does this constraint affect:\n",
    "\n",
    "- The form of the Bellman optimality equation?\n",
    "- The optimal value function, $V^*(s)$?\n",
    "- The structure of the optimal policy, $\\pi^*(s)$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.8.1' requires the ipykernel package.\n",
      "\u001b[1;31mRun the following command to install 'ipykernel' into the Python environment. \n",
      "\u001b[1;31mCommand: '/usr/bin/python3 -m pip install ipykernel -U --user --force-reinstall'"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "We know the Bellman equation takes form $$V^*(s) = max[R(s,a) + γ∑P(s'|s,a)V^*(s')]$$\n",
    "\n",
    "Given this, we can calculate:\n",
    "\n",
    "$$ R(s,a) = (1-a)(1+a) + (a(1-a)) = 1+a-2a^2,  γ∑P(s'|s,a)V^*(s') = γ(aV^*(s+1)+ (1-a)V^*(s) ) $$\n",
    "\n",
    "Subbing this all in:\n",
    "\n",
    "$$V^*(s) = max[1+a-2a^2 + γ(aV^*(s+1)+ (1-a)V^*(s))]$$\n",
    "\n",
    "Expanding:\n",
    "\n",
    "$$V^*(s) = 1 +a - 2a² + γaV^*(s+1) + γV^*(s) - γaV^*(s)$$\n",
    "\n",
    "To find the maximum, we take the derivative with respect to a and set it equal to zero:\n",
    "$$1 -4a + γV^*(s+1) - γV^*(s) = 0$$\n",
    "\n",
    "Solving for a:\n",
    "$$a^* = 1/4 + (γ/4)(V^*(s+1) - V^*(s)) $$\n",
    "\n",
    "Finally, because the rewards and choices remain the same from state to state, we should expect $V^*(s) = V^*(s+1)$, meaning that after substituting in γ\n",
    "\n",
    "$$a^* = 1/4, V^*(s) = 2.25$$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "\n",
    "\n",
    "From our previous analysis, we found that in every state:\n",
    "\n",
    "The optimal action $a^* = 1/4$. This was true regardless of which state we were in, given state invariance.\n",
    "\n",
    "Therefore, the optimal deterministic policy π^*(s) would be:\n",
    "$$π^*(s) = 1/4 \\forall  s ∈ S$$\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "#### Bellman Optimality Equation Change:\n",
    "<span style=\"color:red\">\n",
    "\n",
    "The equation $V^*(s) = max[1 - 2a^2 + γ(aV^*(s+1) + (1-a)V^*(s))]$ remains the same, but now the maximization is over a ∈ [0,1/s] instead of a ∈ [0,1]\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "#### Optimal Value Function Change:\n",
    "<span style=\"color:red\">\n",
    "\n",
    "Since we previously found that $a^* = 1/4$ was optimal (giving $V^*(s) = 2.25$), we expect this value to stop becoming available for states that are $>4$ (since 0 ∈ [0,1/s] for all s), so our previous solution will cease to work after the first states. Without solving, we can say that we'd expect the optimal value function to slowly decrease over time as the action space is bounded more tightly.\n",
    "\n",
    "</span>\n",
    "\n",
    "\n",
    "#### Optimal Policy Change:\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "The optimal policy π*(s):\n",
    "Carrying on from above, our optimal policy is no longer constant for all states; beyond the first four states, the optimal policy will decrease continuously and change to meet the demands from the action space restriction. A closed form solution cannot be found without initial conditions.\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3: Frog in a Pond (Led by Ohm)\n",
    "\n",
    "Consider an array of $n+1$ lilypads on a pond, numbered $0$ to $n$. A frog sits on a lilypad other than the lilypads numbered $0$ or $n$. When on lilypad $i$ ($1 \\leq i \\leq n-1$), the frog can croak one of two sounds: **A** or **B**. \n",
    "\n",
    "- If it croaks **A** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to lilypad $i-1$ with probability $\\frac{i}{n}$.\n",
    "  - It is thrown to lilypad $i+1$ with probability $\\frac{n-i}{n}$.\n",
    "  \n",
    "- If it croaks **B** when on lilypad $i$ ($1 \\leq i \\leq n-1$):\n",
    "  - It is thrown to one of the lilypads $0, \\ldots, i-1, i+1, \\ldots, n$ with uniform probability $\\frac{1}{n}$.\n",
    "\n",
    "A snake, perched on lilypad $0$, will eat the frog if it lands on lilypad $0$. The frog can escape the pond (and hence, escape the snake!) if it lands on lilypad $n$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "What should the frog croak when on each of the lilypads $1, 2, \\ldots, n-1$, in order to maximize the probability of escaping the pond (i.e., reaching lilypad $n$ before reaching lilypad $0$)? \n",
    "\n",
    "Although there are multiple ways to solve this problem, we aim to solve it by modeling it as a **Markov Decision Process (MDP)** and identifying the **Optimal Policy**.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): MDP Modeling\n",
    "\n",
    "Express the frog-escape problem as an MDP using clear mathematical notation by defining the following components: \n",
    "\n",
    "- **State Space**: Define the possible states of the MDP. \n",
    "- **Action Space**: Specify the actions available to the frog at each state. \n",
    "- **Transition Function**: Describe the probabilities of transitioning between states for each action. \n",
    "- **Reward Function**: Specify the reward associated with the states and transitions. \n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Python Implementation\n",
    "\n",
    "There is starter code below to solve this problem programatically. Fill in each of the $6$ `TODO` areas in the code. As a reference for the transition probabilities and rewards, you can make use of the example in slide 16/31 from the following slide deck: https://github.com/coverdrive/technical-documents/blob/master/finance/cme241/Tour-MP.pdf.\n",
    "\n",
    "Write Python code that:\n",
    "\n",
    "- Models this MDP.\n",
    "- Solves the **Optimal Value Function** and the **Optimal Policy**.\n",
    "\n",
    "Feel free to use/adapt code from the textbook. Note, there are other libraries that are needed to actually run this code, so running it will not do anything. Just fill in the code so that it could run assuming that the other libraries are present.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Visualization and Analysis\n",
    "\n",
    "After running the code, we observe the following graphs for $n=3$, $n=10$, and $n=25$:\n",
    "\n",
    "![FrogGraphs](./Figures/frogGraphs.png)\n",
    "\n",
    "What patterns do you observe for the **Optimal Policy** as you vary $n$ from $3$ to $25$? When the frog is on lilypad $13$ (with $25$ total), what action should the frog take? Is this action different than the action the frog should take if it is on lilypad $1$?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "#### State Space:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "$ S = {0, 1, ..., n} \\newline$\n",
    "$N = {1, 2, ..., n-1}$  # Non-terminal states $\\newline$\n",
    "$T = {0, n}           $ #Terminal states (eaten or escaped) \n",
    "\n",
    "</span>\n",
    "\n",
    "#### Action Space:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "Action Space $A(s) = \\{A,B\\}$ for states $\\{1,...,n-1\\}$ and $0$ for states $\\{0,n\\}$\n",
    "\n",
    "</span>\n",
    "\n",
    "#### Transition Function:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "For state i ∈ N and action A:\n",
    "$P(i, A, i-1) = i/n $      # Probability of moving left $\\newline$\n",
    "$P(i, A, i+1) = (n-i)/n $  # Probability of moving right $\\newline$\n",
    "$P(i, A, j) = 0   $        # For all other j from $(0,n)$\n",
    "\n",
    "For state i ∈ N and action B: $\\newline$\n",
    "$P(i, B, j) = 1/n  $        # For all j ∈ {0,...,i-1,i+1,...,n}\n",
    "</span>\n",
    "\n",
    "#### Reward Function:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "R(n) = 1    # Reward for escaping $\\newline$\n",
    "R(0) = -1    # Reward for getting eaten $\\newline$\n",
    "R(i) = 0    # For all other states i ∈ N $\\newline$\n",
    "\n",
    "γ = 1       # No discounting needed (episodic problem)\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MDPRefined = dict\n",
    "def get_lily_pads_mdp(n: int) -> MDPRefined:\n",
    "    data = {\n",
    "        i: {\n",
    "            'A': {\n",
    "                i - 1: 1/n, # TODO: fill in with the correct transition probabilities\n",
    "                i + 1: (n-i)/n, # TODO: fill in with the correct transition probabilities\n",
    "            },\n",
    "            'B': {\n",
    "                j: 1/n for j in range(0, n + 1) # TODO: fill in with the correct transition probabilities\n",
    "            }\n",
    "        } for i in range(1, n)\n",
    "    }\n",
    "    data[0] = {0: 1.0} # TODO: this is the initial state, so what would be the correct transition probabilities?\n",
    "    data[n] = {n: 1.0} # TODO: similarly, this is the terminal state, so what would be the correct transition probabilities?\n",
    "\n",
    "    gamma = 1.0\n",
    "    return MDPRefined(data, gamma)\n",
    "\n",
    "Mapping = dict\n",
    "def direct_bellman(n: int) -> Mapping[int, float]:\n",
    "    vf = [0.5] * (n + 1)\n",
    "    vf[0] = 0.\n",
    "    vf[n] = 0.\n",
    "    tol = 1e-8\n",
    "    epsilon = tol * 1e4\n",
    "    while epsilon >= tol:\n",
    "        old_vf = [v for v in vf]\n",
    "        for i in range(1, n):\n",
    "            vf[i] = max(sum(prob * old_vf[next_state] for next_state, prob in get_lily_pads_mdp(n)[i]['A'].items()),  # E[val] for action A\n",
    "                        sum(prob * old_vf[next_state] for next_state, prob in get_lily_pads_mdp(n)[i]['B'].items())   # E[val] for action B\n",
    "           )    \n",
    "            # TODO: fill in with the Bellman update\n",
    "        epsilon = max(abs(old_vf[i] - v) for i, v in enumerate(vf))\n",
    "    return {v: f for v, f in enumerate(vf)}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "At most positions, Action A (red line) is optimal since its Q* value is higher than Action B (blue line). This makes intuitive sense because Action A gives the frog more controlled movement - it can only move one step forward or backward with carefully calculated probabilities that depend on its position.\n",
    "$\\newline$\n",
    "When n=25 and the frog is on lilypad 13, we can see from the bottom graph that Action A is optimal. At position 13, the red line (Q* for Action A) is clearly above the blue line (Q* for Action B), with a Q* value around 0.68.\n",
    "This is different from the action the frog should take at lilypad 1. Looking at position 1 on the n=25 graph, we see that Action B is actually optimal there (blue line is above red line). This difference makes sense strategically: at lilypad 1, the frog is in a precarious position near the snake, so taking Action B to potentially make a larger jump toward safety might be worth the risk. However, at lilypad 13, the frog is in the middle of the pond and can make more controlled movements with Action A to gradually progress toward the escape point.\n",
    "The graphs also reveal that as n increases, the optimal policy tends to favor Action A more consistently across the middle positions, with Action B only being optimal very close to the starting position. This suggests that controlled, gradual movement becomes more important in larger ponds, except when the frog is in immediate danger near the snake.\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4: Manual Value Iteration (Led by Benjamin)\n",
    "\n",
    "Consider a simple MDP with $\\mathcal{S} = \\{s_1, s_2, s_3\\}, \\mathcal{T} = \\{s_3\\}, \\mathcal{A} = \\{a_1, a_2\\}$. The State Transition Probability function  \n",
    "$$\\mathcal{P}: \\mathcal{N} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0, 1]$$  \n",
    "is defined as:  \n",
    "$$\\mathcal{P}(s_1, a_1, s_1) = 0.25, \\mathcal{P}(s_1, a_1, s_2) = 0.65, \\mathcal{P}(s_1, a_1, s_3) = 0.1$$  \n",
    "$$\\mathcal{P}(s_1, a_2, s_1) = 0.1, \\mathcal{P}(s_1, a_2, s_2) = 0.4, \\mathcal{P}(s_1, a_2, s_3) = 0.5$$  \n",
    "$$\\mathcal{P}(s_2, a_1, s_1) = 0.3, \\mathcal{P}(s_2, a_1, s_2) = 0.15, \\mathcal{P}(s_2, a_1, s_3) = 0.55$$  \n",
    "$$\\mathcal{P}(s_2, a_2, s_1) = 0.25, \\mathcal{P}(s_2, a_2, s_2) = 0.55, \\mathcal{P}(s_2, a_2, s_3) = 0.2$$  \n",
    "\n",
    "The Reward Function  \n",
    "$$\\mathcal{R}: \\mathcal{N} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$$  \n",
    "is defined as:  \n",
    "$$\\mathcal{R}(s_1, a_1) = 8.0, \\mathcal{R}(s_1, a_2) = 10.0$$  \n",
    "$$\\mathcal{R}(s_2, a_1) = 1.0, \\mathcal{R}(s_2, a_2) = -1.0$$  \n",
    "\n",
    "Assume a discount factor of $\\gamma = 1$.\n",
    "\n",
    "### Problem Statement\n",
    "\n",
    "Your task is to determine an Optimal Deterministic Policy **by manually working out** (not with code) the first two iterations of the Value Iteration algorithm.\n",
    "\n",
    "---\n",
    "\n",
    "### Subquestions\n",
    "\n",
    "#### Part (A): 2 Iterations\n",
    "\n",
    "1. Initialize the Value Function for each state to be its $\\max$ (over actions) reward, i.e., we initialize the Value Function to be $v_0(s_1) = 10.0, v_0(s_2) = 1.0, v_0(s_3) = 0.0$. Then manually calculate $q_k(\\cdot, \\cdot)$ and $v_k(\\cdot)$ from $v_{k - 1}(\\cdot)$ using the Value Iteration update, and then calculate the greedy policy $\\pi_k(\\cdot)$ from $q_k(\\cdot, \\cdot)$ for $k = 1$ and $k = 2$ (hence, 2 iterations).\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (B): Argument\n",
    "\n",
    "1. Now argue that $\\pi_k(\\cdot)$ for $k > 2$ will be the same as $\\pi_2(\\cdot)$. *Hint*: You can make the argument by examining the structure of how you get $q_k(\\cdot, \\cdot)$ from $v_{k-1}(\\cdot)$. With this argument, there is no need to go beyond the two iterations you performed above, and so you can establish $\\pi_2(\\cdot)$ as an Optimal Deterministic Policy for this MDP.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (C): Policy Evaluation\n",
    "\n",
    "1. Using the policy $\\pi_2(\\cdot)$, compute the exact value function $V^{\\pi_2}(s)$ for all $s\\in S$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Part (D): Sensitivity Analysis\n",
    "\n",
    "Assume the reward for $R(s_1, a_2)$ is modified to $11.0$ instead of $10.0$.\n",
    "\n",
    "1. Perform one iteration of Value Iteration starting from the initialized value function $v_0(s)$, where $v_0(s)$ remains the same as in the original problem.\n",
    "2. Determine whether this change impacts the Optimal Deterministic Policy $\\pi(\\cdot)$. If it does, explain why.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (A) Answer\n",
    "\n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "#### Iteration 1\n",
    "\n",
    "q₁(s₁,a₁) = R(s₁,a₁) + γ∑ₛ′P(s₁,a₁,s′)v₀(s′) $\\newline$\n",
    "\n",
    "q₁(s₁,a₁) = 8.0 + 1[0.25 × 10.0 + 0.65 × 1.0 + 0.1 × 0.0]\n",
    "= 8.0 + [2.5 + 0.65 + 0]\n",
    "= 8.0 + 3.15\n",
    "= 11.15 $\\newline$\n",
    "\n",
    "q₁(s₁,a2) = R(s₁,a2) + γ∑ₛ′P(s₁,a2,s′)v₀(s′) $\\newline$\n",
    "q₁(s₁,a2) = 10.0 + 1[0.1 × 10.0 + 0.4 × 1.0 + 0.5 × 0.0]\n",
    "= 10.0 + [1.0 + 0.4 + 0]\n",
    "= 10+1.4\n",
    "= 11.4 $\\newline$\n",
    "\n",
    "From this, we know that $v_1(s_1) = 11.4$ and $\\pi_1(s_1) = a_2$ $\\newline$\n",
    "\n",
    "q₁(s2,a1) = R(s2,a₁) + γ∑ₛ′P(s2,a₁,s′)v₀(s′) $\\newline$\n",
    "\n",
    "q₁(s2,a₁) = 1.0 + 1[0.30 × 10.0 + 0.15 × 1.0 + 0.55 × 0.0]\n",
    "= 1.0 + [3.0 + 0.15 + 0]\n",
    "= 1.0 + 3.15\n",
    "= 4.15 $\\newline$\n",
    "\n",
    "q₁(s2,a2) = R(s2,a2) + γ∑ₛ′P(s2,a2,s′)v₀(s′) $\\newline$\n",
    "\n",
    "q₁(s2,a2) = -1.0 + 1[0.25 × 10.0 + 0.55 × 1.0 + 0.2 × 0.0]\n",
    "= -1.0 + [2.5 + 0.55 + 0]\n",
    "= -1.0 + 3.05\n",
    "= 2.05 $\\newline$\n",
    "\n",
    "From this, we know that $v_1(s_2) = 4.15$ and $\\pi_1(s_2) = a_1$ $\\newline$\n",
    "\n",
    "\n",
    "#### Iteration 2\n",
    "\n",
    "q₂(s₁,a₁) = 8.0 + 1[0.25 × 11.4 + 0.65 × 4.15 + 0.1 × 0.0]\n",
    "= 8.0 + [2.85 + 2.70 + 0]\n",
    "= 13.55 $\\newline$\n",
    "q₂(s₁,a₂) = 10.0 + 1[0.1 × 11.4 + 0.4 × 4.15 + 0.5 × 0.0]\n",
    "= 10.0 + [1.14 + 1.66 + 0]\n",
    "= 12.80 $\\newline$\n",
    "\n",
    "From this, we know that $v_2(s_1) = 13.55$ and $\\pi_2(s_1) = a_1$ $\\newline$\n",
    "\n",
    "q₂(s₂,a₁) = 1.0 + 1[0.3 × 11.4 + 0.15 × 4.15 + 0.55 × 0.0]\n",
    "= 1.0 + [3.42 + 0.62 + 0]\n",
    "= 5.04 $\\newline$\n",
    "q₂(s₂,a₂) = -1.0 + 1[0.25 × 11.4 + 0.55 × 4.15 + 0.2 × 0.0]\n",
    "= -1.0 + [2.85 + 2.28 + 0]\n",
    "= 4.13 $\\newline$\n",
    "\n",
    "From this, we know that $v_2(s_2) = 5.04$ and $\\pi_2(s_2) = a_1$ $\\newline$\n",
    "\n",
    "\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (B) Answer:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "In each iteration k, we calculate qₖ using three components:\n",
    "\n",
    "The immediate reward R(s,a)\n",
    "The transition probabilities P(s,a,s')\n",
    "The previous iteration's values vₖ₋₁(s')\n",
    "\n",
    "The immediate rewards and transition probabilities never change - they're fixed properties of our MDP. The only thing that changes between iterations is the v values we use.\n",
    "By iteration 2, we discovered that a₁ is the optimal action for both states. This means that when we calculate q₃, we'll be using v₂ values that already reflect this understanding of optimal behavior. These v₂ values have already \"learned\" that staying in play and having opportunities to accumulate more rewards (through a₁) is better than taking actions that might give higher immediate rewards but lead more often to termination.\n",
    "For the policy to change in iteration 3 or beyond, we would need to discover some new path or combination of states and actions that gives better value than what we found in iteration 2. However, since we're in a finite MDP with no cycles (due to s₃ being terminal), and we've already looked two steps ahead, we've effectively seen all possible future consequences of our actions.\n",
    "Therefore, π₂ must be the optimal deterministic policy - we won't discover any better sequence of actions in future iterations.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (C) Answer:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "We begin with two equations:\n",
    "\n",
    "$V^{π₂}(s₁) = 8.0 + 0.25V^{π₂}(s₁) + 0.65V^{π₂}(s₂) + 0.1 × 0$ $\\newline$\n",
    "$V^{π₂}(s₂) = 1.0 + 0.3V^{π₂}(s₁) + 0.15V^{π₂}(s₂) + 0.55 × 0$ $\\newline$\n",
    "Simplifying these equations:\n",
    "$V^{π₂}(s₁) = 8.0 + 0.25V^{π₂}(s₁) + 0.65V^{π₂}(s₂)$ $\\newline$\n",
    "$V^{π₂}(s₂) = 1.0 + 0.3V^{π₂}(s₁) + 0.15V^{π₂}(s₂)$ $\\newline$\n",
    "Solving for V^{π₂}(s₁) in the first equation:\n",
    "$V^{π₂}(s₁) - 0.25V^{π₂}(s₁) = 8.0 + 0.65V^{π₂}(s₂)$ $\\newline$\n",
    "$0.75V^{π₂}(s₁) = 8.0 + 0.65V^{π₂}(s₂)$ $\\newline$\n",
    "$V^{π₂}(s₁) = \\frac{8.0 + 0.65V^{π₂}(s₂)}{0.75}$ $\\newline$\n",
    "$V^{π₂}(s₁) = 10.67 + 0.87V^{π₂}(s₂)$ $\\newline$\n",
    "Substituting this into the second equation:\n",
    "$V^{π₂}(s₂) = 1.0 + 0.3(10.67 + 0.87V^{π₂}(s₂)) + 0.15V^{π₂}(s₂)$ $\\newline$\n",
    "$V^{π₂}(s₂) = 1.0 + 3.20 + 0.26V^{π₂}(s₂) + 0.15V^{π₂}(s₂)$ $\\newline$\n",
    "$V^{π₂}(s₂) = 4.20 + 0.41V^{π₂}(s₂)$ $\\newline$\n",
    "$0.59V^{π₂}(s₂) = 4.20$ $\\newline$\n",
    "$V^{π₂}(s₂) = 7.12$ $\\newline$\n",
    "$V^{π₂}(s₁) = 10.67 + 0.87(7.12)$ $\\newline$\n",
    "$V^{π₂}(s₁) = 10.67 + 6.19 = 16.86$ $\\newline$\n",
    "Therefore, our final values are:\n",
    "$V^{π₂}(s₁) = 16.86$ $\\newline$\n",
    "$V^{π₂}(s₂) = 7.12$ $\\newline$\n",
    "$V^{π₂}(s₃) = 0$ (terminal state) $\\newline$\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part (D) Answer\n",
    "\n",
    "#### Value Iteration:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "q₁(s₁,a₁) = R(s₁,a₁) + γ∑ₛ′P(s₁,a₁,s′)v₀(s′) $\\newline$\n",
    "\n",
    "q₁(s₁,a₁) = 8.0 + 1[0.25 × 10.0 + 0.65 × 1.0 + 0.1 × 0.0]\n",
    "= 8.0 + [2.5 + 0.65 + 0]\n",
    "= 8.0 + 3.15\n",
    "= 11.15 $\\newline$\n",
    "\n",
    "q₁(s₁,a2) = R(s₁,a2) + γ∑ₛ′P(s₁,a2,s′)v₀(s′) $\\newline$\n",
    "q₁(s₁,a2) = 11.0 + 1[0.1 × 10.0 + 0.4 × 1.0 + 0.5 × 0.0]\n",
    "= 11.0 + [1.0 + 0.4 + 0]\n",
    "= 11+1.4\n",
    "= 12.4 $\\newline$\n",
    "\n",
    "From this, we know that $v_1(s_1) = 12.4$ and $\\pi_1(s_1) = a_2$ $\\newline$\n",
    "\n",
    "q₁(s2,a1) = R(s2,a₁) + γ∑ₛ′P(s2,a₁,s′)v₀(s′) $\\newline$\n",
    "\n",
    "q₁(s2,a₁) = 1.0 + 1[0.30 × 10.0 + 0.15 × 1.0 + 0.55 × 0.0]\n",
    "= 1.0 + [3.0 + 0.15 + 0]\n",
    "= 1.0 + 3.15\n",
    "= 4.15 $\\newline$\n",
    "\n",
    "q₁(s2,a2) = R(s2,a2) + γ∑ₛ′P(s2,a2,s′)v₀(s′) $\\newline$\n",
    "\n",
    "q₁(s2,a2) = -1.0 + 1[0.25 × 10.0 + 0.55 × 1.0 + 0.2 × 0.0]\n",
    "= -1.0 + [2.5 + 0.55 + 0]\n",
    "= -1.0 + 3.05\n",
    "= 2.05 $\\newline$\n",
    "\n",
    "From this, we know that $v_1(s_2) = 4.15$ and $\\pi_1(s_2) = a_1$ $\\newline$\n",
    "\n",
    "\n",
    "\n",
    "</span>\n",
    "\n",
    "#### Optimal Deterministic Policy:  \n",
    "\n",
    "<span style=\"color:red\">\n",
    "\n",
    "After one iteration of Value Iteration with R(s₁,a₂) = 11.0 and the same initial values v₀(s₁) = 10.0, v₀(s₂) = 1.0, v₀(s₃) = 0.0, we get:\n",
    "q₁(s₁,a₁) = 11.15\n",
    "q₁(s₁,a₂) = 12.4\n",
    "q₁(s₂,a₁) = 4.15\n",
    "q₁(s₂,a₂) = 2.05\n",
    "This change in reward does impact the optimal deterministic policy. In the original problem, while a₂ appeared optimal after one iteration, further iterations revealed that a₁'s better ability to keep the process in play made it the truly optimal choice for s₁. However, with R(s₁,a₂) increased to 11.0, the gap between q₁(s₁,a₂) and q₁(s₁,a₁) has widened (from 0.25 to 1.25). Given how sensitive our original optimal policy was to small differences in value, this increase in immediate reward for a₂ is enough to overcome the long-term advantage of a₁'s lower termination probability, making a₂ the optimal action for s₁ even after considering future consequences.\n",
    "\n",
    "To demonstrate (asked AI to take iteration 2):\n",
    "\n",
    "For s₁ with a₁:\n",
    "q₂(s₁,a₁) = 8.0 + [0.25 × 12.4 + 0.65 × 4.15 + 0.1 × 0]\n",
    "= 8.0 + [3.1 + 2.70 + 0]\n",
    "= 13.8\n",
    "For s₁ with a₂:\n",
    "q₂(s₁,a₂) = 11.0 + [0.1 × 12.4 + 0.4 × 4.15 + 0.5 × 0]\n",
    "= 11.0 + [1.24 + 1.66 + 0]\n",
    "= 13.9\n",
    "For s₂ with a₁:\n",
    "q₂(s₂,a₁) = 1.0 + [0.3 × 12.4 + 0.15 × 4.15 + 0.55 × 0]\n",
    "= 1.0 + [3.72 + 0.62 + 0]\n",
    "= 5.34\n",
    "For s₂ with a₂:\n",
    "q₂(s₂,a₂) = -1.0 + [0.25 × 12.4 + 0.55 × 4.15 + 0.2 × 0]\n",
    "= -1.0 + [3.1 + 2.28 + 0]\n",
    "= 4.38 $\\newline$\n",
    "This gives us:\n",
    "v₂(s₁) = max{13.8, 13.9} = 13.9, so π₂(s₁) = a₂ $\\newline$\n",
    "v₂(s₂) = max{5.34, 4.38} = 5.34, so π₂(s₂) = a₁ $\\newline$\n",
    "\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5: Fixed-Point and Policy Evaluation True/False Questions (Led by Stephen)\n",
    "\n",
    "### Recall Section: Key Formulas and Definitions\n",
    "\n",
    "#### Bellman Optimality Equation\n",
    "The Bellman Optimality Equation for state-value functions is:\n",
    "$$\n",
    "V^*(s) = \\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^*(s') \\right].\n",
    "$$\n",
    "For action-value functions:\n",
    "$$\n",
    "Q^*(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') \\max_{a'} Q^*(s', a').\n",
    "$$\n",
    "\n",
    "#### Contraction Property\n",
    "The Bellman Policy Operator $B^\\pi$ is a contraction under the $L^\\infty$-norm:\n",
    "$$\n",
    "\\|B^\\pi(X) - B^\\pi(Y)\\|_\\infty \\leq \\gamma \\|X - Y\\|_\\infty.\n",
    "$$\n",
    "This guarantees convergence to a unique fixed point.\n",
    "\n",
    "#### Policy Iteration\n",
    "Policy Iteration alternates between:\n",
    "1. **Policy Evaluation**: Compute $V^\\pi$ for the current policy $\\pi$.\n",
    "2. **Policy Improvement**: Generate a new policy $\\pi'$ by setting:\n",
    "   $$\n",
    "   \\pi'(s) = \\arg\\max_a \\left[ R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s') \\right].\n",
    "   $$\n",
    "\n",
    "#### Discounted Return\n",
    "The discounted return from time step $t$ is:\n",
    "$$\n",
    "G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i,\n",
    "$$\n",
    "where $\\gamma \\in [0, 1)$ is the discount factor.\n",
    "\n",
    "### True/False Questions (Provide Justification)\n",
    "\n",
    "1. **True/False**: If $Q^\\pi(s, a) = 5$, $P(s, a, s') = 0.5$ for $s' \\in \\{s_1, s_2\\}$, and the immediate reward $R(s, a)$ increases by $2$, the updated action-value function $Q^\\pi(s, a)$ also increases by $2$.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "2. **True/False**: For a discount factor $\\gamma = 0.9$, the discounted return for rewards $R_1 = 5, R_2 = 3, R_3 = 1$ is greater than $6$.\n",
    "\n",
    "---\n",
    "\n",
    "3. **True/False**: The Bellman Policy Operator $B^\\pi(V) = R^\\pi + \\gamma P^\\pi \\cdot V$ satisfies the contraction property for all $\\gamma \\in [0, 1)$, ensuring a unique fixed point.\n",
    "\n",
    "---\n",
    "\n",
    "4. **True/False**: In Policy Iteration, the Policy Improvement step guarantees that the updated policy $\\pi'$ will always perform strictly better than the previous policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "5. **True/False**: If $Q^\\pi(s, a) = 10$ for all actions $a$ in a state $s$, then the corresponding state-value function $V^\\pi(s) = 10$, regardless of the policy $\\pi$.\n",
    "\n",
    "---\n",
    "\n",
    "6. **True/False**: The discounted return $G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i$ converges to a finite value for any sequence of bounded rewards if $\\gamma < 1$.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Answers (Provide justification, brief explanations are fine)\n",
    "\n",
    "#### Question 1:  \n",
    "\n",
    "<span>\n",
    "   Answer: True \n",
    "\n",
    "   Explanation: The action-value function is given by:\n",
    "\n",
    "   $$\n",
    "   Q^\\pi(s, a) = R(s, a) + \\gamma \\sum_{s'} P(s, a, s') V^\\pi(s').\n",
    "   $$\n",
    "\n",
    "   If R(s, a) increases by 2, this will directly add 2 to $Q^{π}(s, a)$, as the other terms $\\gamma \\sum_{s'} P(s, a, s') V^\\pi(s')$ remain unchanged. Hence, the statement is true.\n",
    "\n",
    "</span>\n",
    "\n",
    "#### Question 2:  \n",
    "\n",
    "<span>\n",
    "   Answer: True\n",
    "\n",
    "   Explanation: The discounted return equation is given by:\n",
    "   $$\n",
    "    G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i\n",
    "   $$\n",
    "   Using the information provided by the in question and assuming we are looking at the discount from period 0, hence calculating $G_0$, we could write the following: \n",
    "   $$\n",
    "   G_t = R_1 + \\gamma R_2 + \\gamma^2 R_3 = 5 + 0.9 \\cdot 3 + 0.9^2 \\cdot 1 = 5 + 2.7 + 0.81 = 8.51 > 6,\n",
    "   $$\n",
    "   Therefore, the statement is True. \n",
    "</span>\n",
    "\n",
    "#### Question 3:  \n",
    "\n",
    "<span>\n",
    "   Answer: True  \n",
    "\n",
    "   Explaination: by the contraction property defintion \n",
    "The operator $B^\\pi$ is a contraction if, for any two value functions X and Y:\n",
    "$$\n",
    "\\| B^\\pi(X) - B^\\pi(Y) \\|_\\infty \\leq \\gamma \\| X - Y \\|_\\infty.\n",
    "$$\n",
    "Here, $\\| \\cdot \\|_\\infty$ represents the maximum absolute difference over all states, defined as:\n",
    "$$\n",
    "\\| X - Y \\|_\\infty = \\max_s | X(s) - Y(s) |.\n",
    "$$\n",
    "\n",
    "Consider two value functions X and Y. Apply the Bellman Policy Operator $B^\\pi$ to both:\n",
    "$$\n",
    "B^\\pi(X)(s) = R^\\pi(s) + \\gamma \\sum_{s'} P^\\pi(s, s') X(s'),\n",
    "$$\n",
    "$$\n",
    "B^\\pi(Y)(s) = R^\\pi(s) + \\gamma \\sum_{s'} P^\\pi(s, s') Y(s').\n",
    "$$\n",
    "\n",
    "Taking the difference:\n",
    "$$\n",
    "B^\\pi(X)(s) - B^\\pi(Y)(s) = \\gamma \\sum_{s'} P^\\pi(s, s') \\left[ X(s') - Y(s') \\right].\n",
    "$$\n",
    "\n",
    "The absolute value of the difference is:\n",
    "$$\n",
    "| B^\\pi(X)(s) - B^\\pi(Y)(s) | = \\gamma \\left| \\sum_{s'} P^\\pi(s, s') \\left[ X(s') - Y(s') \\right] \\right|.\n",
    "$$\n",
    "\n",
    "Since $P^\\pi(s, s')$ is a probability distribution (non-negative and sums to 1), the weighted sum is bounded by the maximum difference across all states:\n",
    "$$\n",
    "| B^\\pi(X)(s) - B^\\pi(Y)(s) | \\leq \\gamma \\max_{s'} | X(s') - Y(s') |.\n",
    "$$\n",
    "\n",
    "This holds for all states $s$, so taking the maximum over $s$:\n",
    "$$\n",
    "\\| B^\\pi(X) - B^\\pi(Y) \\|_\\infty \\leq \\gamma \\| X - Y \\|_\\infty.\n",
    "$$\n",
    "\n",
    "Given $\\gamma < 1$ and $\\gamma >= 0$, the operator $B^\\pi$ shrinks the distance between any two value functions. This is the contraction property, which ensures that\n",
    "$B^\\pi$ converges to a unique fixed point. Therefore the statement is True.\n",
    "\n",
    "</span>\n",
    "\n",
    "#### Question 4:  \n",
    "\n",
    "<span>\n",
    "  Answer: False\n",
    "\n",
    "  Explanation: The policy improvement guarantees the new policy $\\pi$ is at least as good as $\\pi$, but it can be equal to $\\pi$ if $\\pi$ is already optimal. From the statement it states \"always perform strictly better\", which would exclude \n",
    "  the case where the \"old\" policy is already optimal (meaning there is no strict improvement possible). Hence, this means the statement is False.\n",
    "</span>\n",
    "\n",
    "#### Question 5:  \n",
    "\n",
    "<span>\n",
    "   Answer: True\n",
    "\n",
    "   Explanation: The state-value function is:\n",
    "   $$\n",
    "   V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s, a).\n",
    "   $$\n",
    "   \n",
    "   If $Q^\\pi(s, a) = 10$  for all a, then regardless of the policy $\\pi$, the weighted sum of $Q^\\pi(s, a)$ values will be 10，since $\\sum_{a \\in A} \\pi(a|s) = 1$. Hence, the statement is True.\n",
    "</span>\n",
    "\n",
    "#### Question 6:  \n",
    "\n",
    "<span>\n",
    "   Answer: True\n",
    "\n",
    "   Explanation: If the rewards are bounded by some constant $M$, i.e., $\\lvert R_i \\rvert \\leq M$ for all $i$, and given the discounted return:\n",
    "   $$\n",
    "    G_t = \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i\n",
    "   $$\n",
    "   must converge for $\\gamma < 1$. Taking absolute values:\n",
    "   $$\n",
    "    \\bigl\\lvert G_t \\bigr\\rvert = \\left\\lvert \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} R_i \\right\\rvert\n",
    "    \\;\\leq\\; \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} \\lvert R_i \\rvert\n",
    "    \\;\\leq\\; M \\sum_{i=t+1}^\\infty \\gamma^{i-t-1}.\n",
    "   $$\n",
    "\n",
    "   By re-indexing the sum (let $k = i - (t+1)$) so we could simplify it and formulate it to the geometric sum, we obtain:\n",
    "   $$\n",
    "    \\sum_{i=t+1}^\\infty \\gamma^{i-t-1} = \\sum_{k=0}^\\infty \\gamma^k = \\frac{1}{1-\\gamma}, \\quad \\text{for } 0 \\leq \\gamma < 1.\n",
    "   $$\n",
    "\n",
    "   Thus:\n",
    "   $$\n",
    "    \\bigl\\lvert G_t \\bigr\\rvert \\;\\leq\\; M \\cdot \\frac{1}{1-\\gamma},\n",
    "   $$\n",
    "   which is finite as it cannot blow up to infinity as the partial sums of $G_t$ is contained within a finite range. Hence, the infinite sum of discounted rewards converges, therefore True.\n",
    "</span>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
